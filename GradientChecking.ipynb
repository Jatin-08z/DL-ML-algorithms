{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from testCases_copy_2 import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing gradient checking from scratch.\n",
    "\n",
    "We implement gradient-checking to make sure that our backward_propagation implementation is correct.\n",
    "\n",
    "How to use difference formula to check our backward_propagation implementation.\n",
    "\n",
    "Results acheived from backward_propagation should be similar to those acheived by difference formula.\n",
    "\n",
    "Identify which parameters gradient was computed erroneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula that we will use to verify if our gradients are correct\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ is what you want to make sure you're computing correctly. \n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "- You can compute $J(\\theta + \\varepsilon)$ and $J(\\theta - \\varepsilon)$ (in the case that $\\theta$ is a real number), since you're confident your implementation for $J$ is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    \n",
    "    # x -- a real valued input\n",
    "    # theta -- again a real valued input\n",
    "    \n",
    "    J = theta * x\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "J = forward_propagation(3,1)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    \n",
    "    # J = theta*x \n",
    "    #dtheta = x              derivative of J wrt theta is dtheta as per the nomenclature\n",
    "    dtheta = x;\n",
    "    \n",
    "    #gradient wrt to the input x is not required for in the supervised learning.\n",
    "    return dtheta;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check if the gradients computed by the backward_propagation are correct by computing the gradients from the formula.\n",
    "\n",
    "steps followed\n",
    "\n",
    "1. compute $\\theta^{+} = \\theta + \\varepsilon$\n",
    "2. compute $\\theta^{-} = \\theta + \\varepsilon$\n",
    "3. compute $J(\\theta^{+})$\n",
    "4. compute $J(\\theta^{-})$\n",
    "5. compute gradapprox  $\\frac {J(\\theta^{+}) - J(\\theta^{-})}{2 \\varepsilon}$\n",
    " \n",
    "\n",
    "This equation helps you compute the relative difference between grad_approx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta):\n",
    "    \n",
    "    epsilon = 0.00001\n",
    "    \n",
    "    theta_plus = theta + epsilon\n",
    "    theta_minus = theta - epsilon\n",
    "    J_theta_plus = forward_propagation(x, theta_plus)\n",
    "    J_theta_minus = forward_propagation(x, theta_minus)\n",
    "    \n",
    "    grad_approx = (J_theta_plus - J_theta_minus)/(2*epsilon)\n",
    "    grad = backward_propagation(x, theta)\n",
    "    \n",
    "    #computing the relative difference to see how well is our backward_propagation is doing.\n",
    "    numerator   = np.linalg.norm(grad - grad_approx)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    difference  = numerator/denominator\n",
    "    \n",
    "    if(difference <= 1e-7):\n",
    "        print(\"Gradients are correct \")\n",
    "    else:\n",
    "        print(\"Gradients are wrong \")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are correct \n",
      "difference 7.826683745609991e-12\n"
     ]
    }
   ],
   "source": [
    "print(\"difference \" + str(gradient_check(x, theta)))\n",
    "# I see the two numbers are very very close. so therefore our backward_propagation is computing the correct gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalising gradient Check for neural networks\n",
    "\n",
    "in a general case cost function has more than one 1-D input and the parameters theta cotains many matrices.\n",
    "\n",
    "\n",
    "We will now perform N-dimensional gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18308927],\n",
       "       [-1.1103054 ],\n",
       "       [ 0.43067261],\n",
       "       [ 0.79748214],\n",
       "       [-1.03185549],\n",
       "       [-0.97731724],\n",
       "       [-1.25076174],\n",
       "       [-0.49392573],\n",
       "       [ 0.30469941],\n",
       "       [ 1.84701491],\n",
       "       [ 0.13996798],\n",
       "       [ 0.06679919],\n",
       "       [-0.51184194],\n",
       "       [-1.26582374],\n",
       "       [ 0.17603868],\n",
       "       [-1.06989065],\n",
       "       [-1.10929395],\n",
       "       [ 1.95267687],\n",
       "       [ 1.2991675 ],\n",
       "       [ 0.01806047],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [-2.14957254],\n",
       "       [-0.31620888],\n",
       "       [-0.0873828 ],\n",
       "       [-1.4873786 ],\n",
       "       [ 1.19749602],\n",
       "       [-1.17241613],\n",
       "       [-0.6609025 ],\n",
       "       [ 1.8389223 ],\n",
       "       [-0.36039164],\n",
       "       [-0.15709686],\n",
       "       [ 1.20331536],\n",
       "       [ 0.93063559],\n",
       "       [ 0.98112898],\n",
       "       [ 0.08076176],\n",
       "       [ 0.7074636 ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [-0.21021307],\n",
       "       [ 2.14242413],\n",
       "       [ 0.9195033 ],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just try to see what our dictionary_to_vector function is doing.\n",
    "\n",
    "W1 = np.random.randn(5,4)\n",
    "b1 = np.zeros((5,1))\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.zeros((3,1))\n",
    "W3 = np.random.randn(1,3)\n",
    "b3 = np.zeros((1,1))\n",
    "\n",
    "parameters = {\"W1\":W1, \"b1\":b1, \"W2\":W2, \"b2\":b2, \"W3\":W3, \"b3\":b3}\n",
    "values, _  = dictionary_to_vector(parameters)#the function returns  theta, keys, discard keys\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.18308917, -1.1103053 ,  0.43067271,  0.79748224],\n",
       "        [-1.03185539, -0.97731714, -1.25076164, -0.49392563],\n",
       "        [ 0.30469951,  1.84701501,  0.13996808,  0.06679929],\n",
       "        [-0.51184184, -1.26582364,  0.17603878, -1.06989055],\n",
       "        [-1.10929385,  1.95267697,  1.2991676 ,  0.01806057]]),\n",
       " 'b1': array([[1.e-07],\n",
       "        [1.e-07],\n",
       "        [1.e-07],\n",
       "        [1.e-07],\n",
       "        [1.e-07]]),\n",
       " 'W2': array([[-2.14957244, -0.31620878, -0.0873827 , -1.4873785 ,  1.19749612],\n",
       "        [-1.17241603, -0.6609024 ,  1.8389224 , -0.36039154, -0.15709676],\n",
       "        [ 1.20331546,  0.93063569,  0.98112908,  0.08076186,  0.7074637 ]]),\n",
       " 'b2': array([[1.e-07],\n",
       "        [1.e-07],\n",
       "        [1.e-07]]),\n",
       " 'W3': array([[-0.21021297,  2.14242423,  0.9195034 ]]),\n",
       " 'b3': array([[1.e-07]])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = vector_to_dictionary(values+1e-7)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47, 1), (47, 1))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_values = np.copy(values)\n",
    "parameters_values.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the cost) presented in Figure 3.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- training set for m examples\n",
    "    Y -- labels for m examples \n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (5, 4)\n",
    "                    b1 -- bias vector of shape (5, 1)\n",
    "                    W2 -- weight matrix of shape (3, 5)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost function (logistic cost for one example)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    # Cost\n",
    "    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, parameters = gradient_check_n_test_case()\n",
    "cost, _ = forward_propagation_n(x, y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input datapoint, of shape (input size, 1)\n",
    "    Y -- true \"label\"\n",
    "    cache -- cache output from forward_propagation_n()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) \n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.88514116 -0.75439794]\n",
      " [ 1.25286816  0.51292982]\n",
      " [-0.29809284  0.48851815]]\n",
      "[[ 0.88514116 -0.75439794]\n",
      " [ 1.25286816  0.51292982]\n",
      " [-0.29809284  0.48851815]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(3,2)\n",
    "print(a)\n",
    "\n",
    "b = np.copy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \n",
    "    parameters_values,_ = dictionary_to_vector(parameters)\n",
    "    num_parameters = parameters_values.shape[0] \n",
    "    grad = gradients_to_vector(gradients)\n",
    "    \n",
    "    grad_approx = np.zeros((num_parameters, 1))\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    \n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        #compute J(theta_plus)[i]\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i]  = theta_plus[i] + epsilon\n",
    "        J_plus[i],_ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n",
    "        \n",
    "        #compute J(theta_minus)[i]\n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i]  = theta_minus[i] - epsilon\n",
    "        J_minus[i],_ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))\n",
    "        \n",
    "        grad_approx[i] = (J_plus[i] - J_minus[i]) / (2*epsilon)\n",
    "    \n",
    "    #computing the differences vector.\n",
    "    numerator = np.linalg.norm(grad - grad_approx)                  #numerator vector of the differences vector\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)  #denominator vector of the differences vector \n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if(difference > 2e-7):\n",
    "        print(\"\\033[93m\" +\"there is something worng in your backpropagation! \" +str(difference))\n",
    "    else:\n",
    "        print(\"\\033[92m\" +\"Your backward propagation works perfectly fine! \" +str(difference))\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! 1.1885552035482147e-07\n"
     ]
    }
   ],
   "source": [
    "x, y, parameters = gradient_check_n_test_case()\n",
    "cost, cache = forward_propagation_n(x, y, parameters)\n",
    "gradients = backward_propagation_n(x, y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n",
      "Here is the error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "differences = np.zeros((47,1))\n",
    "for i in range(47):\n",
    "    \n",
    "    numerator = np.linalg.norm(grad[i] - grad_approx[i])\n",
    "    denominator = np.linalg.norm(grad[i]) + np.linalg.norm(grad_approx[i])\n",
    "    if(denominator==0):\n",
    "        print(\"Here is the error\")\n",
    "    differences[i] = numerator/denominator\n",
    "\n",
    "#differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### takeaways\n",
    "\n",
    "1. Gradient checking is slow.\n",
    "2. Gradient checking canot be used with dropout, We usually run it before turning dropout on and then upon reassuring ourselves that our backprop is correct we turn the dropout on.\n",
    "\n",
    "3. We dont run it on every iteration of gradient descent because it is a slow process (see the iterations we run each time, each iter we do \n",
    "    -->copy all parameters as theta plus and theta minus \n",
    "    -->employ forward_propagation 2 times\n",
    "    -->and blah blah operations to calculate the difference.\n",
    "    ) We therefore do it inbetween the iterations of gd a few times like once in 100 or 1000 iterations.\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try resolving the above error and finding out why gradient check is not used during dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
